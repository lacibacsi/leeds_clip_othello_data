{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b03856fc4d154f43ab9baa8c273a5cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b526ff8611b4b5ea6331df4467e430e",
              "IPY_MODEL_8660bc0b5c8a46a487f13439c68cdb3a",
              "IPY_MODEL_453969fa46cb47838526c5ace13cdc37"
            ],
            "layout": "IPY_MODEL_8bccc64cbe7f4900b89cf77ea0ee9313"
          }
        },
        "8b526ff8611b4b5ea6331df4467e430e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa5fd2ebcee34cf6bc1020c4517cbbd1",
            "placeholder": "​",
            "style": "IPY_MODEL_12d02eacd3bb4d89b07d19d174f35533",
            "value": "Generating train split: "
          }
        },
        "8660bc0b5c8a46a487f13439c68cdb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e316377b34b34b0388620e4f62d065fd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34aa62d48ab4470cb06a3e981439ba37",
            "value": 1
          }
        },
        "453969fa46cb47838526c5ace13cdc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1af01cd495d14cb097007e3a4d03f99b",
            "placeholder": "​",
            "style": "IPY_MODEL_284bea6ed57541269df3eb4165866837",
            "value": " 367/0 [00:00&lt;00:00, 4463.84 examples/s]"
          }
        },
        "8bccc64cbe7f4900b89cf77ea0ee9313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5fd2ebcee34cf6bc1020c4517cbbd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d02eacd3bb4d89b07d19d174f35533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e316377b34b34b0388620e4f62d065fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "34aa62d48ab4470cb06a3e981439ba37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1af01cd495d14cb097007e3a4d03f99b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "284bea6ed57541269df3eb4165866837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2554d011f354d8aba288ac7ea22007b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9d3c541360542bca4d161dd79e279c5",
              "IPY_MODEL_65e15494361e4a8287a29bd70d34c5b3",
              "IPY_MODEL_3fc2723716484f73ab4b4201669738b4"
            ],
            "layout": "IPY_MODEL_a8819a39d3b547e198a98a9259c6e50c"
          }
        },
        "d9d3c541360542bca4d161dd79e279c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d330879568e84c72919320b2522a48e4",
            "placeholder": "​",
            "style": "IPY_MODEL_e2b09cc551f34d47966a811274d72b4c",
            "value": "Map: 100%"
          }
        },
        "65e15494361e4a8287a29bd70d34c5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3352769a916945999fd24071e5c58a07",
            "max": 367,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7146d964c8df46ae89c99cf238b05c26",
            "value": 367
          }
        },
        "3fc2723716484f73ab4b4201669738b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd191a60811342bca8845b6007962fa1",
            "placeholder": "​",
            "style": "IPY_MODEL_42c1f687f3ae4b03a20e9b4153e38ee1",
            "value": " 367/367 [00:06&lt;00:00, 45.70 examples/s]"
          }
        },
        "a8819a39d3b547e198a98a9259c6e50c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d330879568e84c72919320b2522a48e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b09cc551f34d47966a811274d72b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3352769a916945999fd24071e5c58a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7146d964c8df46ae89c99cf238b05c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd191a60811342bca8845b6007962fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c1f687f3ae4b03a20e9b4153e38ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data consolidation and cleaning for fine tuning"
      ],
      "metadata": {
        "id": "jbXngGpiEpCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYipPOeC7Gdk",
        "outputId": "bcb22ae0-75e4-4afe-f737-7358491c9863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['othello_rose_0930.csv', 'othello_b_b_0930.csv', 'othello_randy_0930.csv']\n"
          ]
        }
      ],
      "source": [
        "# using the chunks datasets, as sentences do not seem to be very good\n",
        "import os\n",
        "\n",
        "filenames = []\n",
        "for filename in os.listdir('/content/data'):\n",
        "    filenames.append(filename)\n",
        "\n",
        "print(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for filename in filenames:\n",
        "  try:\n",
        "    temp_df = pd.read_csv('/content/data/' + filename)\n",
        "    df = pd.concat([df, temp_df], ignore_index=True)\n",
        "  except:\n",
        "    print('Error reading file: ' + filename)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LuxbUMR6qVc",
        "outputId": "359a42e7-33f7-40f7-9d26-d7b56fc8958f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 input  \\\n",
            "0    Read the following article and answer the ques...   \n",
            "1    Read the following article and answer the ques...   \n",
            "2    builds up step by step from the basics of stra...   \n",
            "3    corners and stable discs. Understanding this m...   \n",
            "4    Read the following article and answer the ques...   \n",
            "..                                                 ...   \n",
            "362  count of disks and see which sequence is “opti...   \n",
            "363  be broken and that corners become worth less l...   \n",
            "364  Read the following article and answer the ques...   \n",
            "365  Read the following context and answer the ques...   \n",
            "366  learn how to play simple endgames correctly.  ...   \n",
            "\n",
            "                                                output  \n",
            "0    Jonathan Cerf has contributed to the book in m...  \n",
            "1             To help people become better at Othello.  \n",
            "2               He is a master at the game of Othello.  \n",
            "3                               not enough information  \n",
            "4                                   introduce the book  \n",
            "..                                                 ...  \n",
            "362                            They like to play games  \n",
            "363                                          satisfied  \n",
            "364                                      a game expert  \n",
            "365  Because white will just play B1 because the B2...  \n",
            "366                                         Randy Fang  \n",
            "\n",
            "[367 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_content_before_question(text):\n",
        "  if isinstance(text, str):\n",
        "    parts = text.split('Question', 1)\n",
        "    if len(parts) > 1:\n",
        "      return parts[0].strip()\n",
        "  return ''\n",
        "\n"
      ],
      "metadata": {
        "id": "o13KkFRtCcaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_context_from_input(row):\n",
        "  if isinstance(row['input'], str) and isinstance(row['Context'], str):\n",
        "    return row['input'].replace(row['Context'], '').strip()\n",
        "  return row['input']\n",
        "\n"
      ],
      "metadata": {
        "id": "tyCUY--TC1TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_question_prefix(text):\n",
        "  if isinstance(text, str) and text.startswith('Question:'):\n",
        "    return text.replace('Question:', '').strip()\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "FL6Uki8rDlWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the order of columns for easier readability\n",
        "\n",
        "cols = ['Context'] + [col for col in df.columns if col != 'Context']\n",
        "#df = df[cols]\n"
      ],
      "metadata": {
        "id": "CTIOh8i0D3MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the output as csv\n",
        "df.to_csv('fine_tuning_set_v2.csv', index=False)"
      ],
      "metadata": {
        "id": "rsSubCUTETGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to jsonl\n",
        "jsonl_filename = 'othello_finetune_v2.jsonl'\n",
        "\n",
        "# Iterate through the rows and write each row as a JSON object to the JSONL file\n",
        "with open(jsonl_filename, 'w') as jsonl_file:\n",
        "    for _, row in df.iterrows():\n",
        "        json_data = row.to_json(orient='columns')\n",
        "        jsonl_file.write(json_data + '\\n')"
      ],
      "metadata": {
        "id": "IOXyi3spEZXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBFNRWNaPuv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization of data"
      ],
      "metadata": {
        "id": "5-QOFVcZGrmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some code and in general inspiraiton from https://medium.com/@balci.pelin/llm-finetuning-410e8a2738ef"
      ],
      "metadata": {
        "id": "pglQjm18Gz6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-jCMj7QH4Jx",
        "outputId": "b894f48b-3849-4705-b86f-030d3e118524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "\n",
        "# using pythin-70m model, sufficient for this exercise\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    if \"input\" in examples and \"output\" in examples:\n",
        "        text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "        text = examples[\"text\"][0]\n",
        "\n",
        "    # Add 0 for short sentences\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    # find the max length after padding, select the min\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048\n",
        "    )\n",
        "\n",
        "    # truncate if the sentence is longer than 2048\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=jsonl_filename, split=\"train\")\n",
        "\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "b03856fc4d154f43ab9baa8c273a5cdf",
            "8b526ff8611b4b5ea6331df4467e430e",
            "8660bc0b5c8a46a487f13439c68cdb3a",
            "453969fa46cb47838526c5ace13cdc37",
            "8bccc64cbe7f4900b89cf77ea0ee9313",
            "fa5fd2ebcee34cf6bc1020c4517cbbd1",
            "12d02eacd3bb4d89b07d19d174f35533",
            "e316377b34b34b0388620e4f62d065fd",
            "34aa62d48ab4470cb06a3e981439ba37",
            "1af01cd495d14cb097007e3a4d03f99b",
            "284bea6ed57541269df3eb4165866837",
            "f2554d011f354d8aba288ac7ea22007b",
            "d9d3c541360542bca4d161dd79e279c5",
            "65e15494361e4a8287a29bd70d34c5b3",
            "3fc2723716484f73ab4b4201669738b4",
            "a8819a39d3b547e198a98a9259c6e50c",
            "d330879568e84c72919320b2522a48e4",
            "e2b09cc551f34d47966a811274d72b4c",
            "3352769a916945999fd24071e5c58a07",
            "7146d964c8df46ae89c99cf238b05c26",
            "bd191a60811342bca8845b6007962fa1",
            "42c1f687f3ae4b03a20e9b4153e38ee1"
          ]
        },
        "id": "7pMINPs8E-hJ",
        "outputId": "779aebc8-b669-4bc4-df94-cb7a9c9fe8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b03856fc4d154f43ab9baa8c273a5cdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/367 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2554d011f354d8aba288ac7ea22007b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tokenized_dataset[\"Context\"][99])\n",
        "\n",
        "print(tokenized_dataset[\"input\"][99])\n",
        "\n",
        "print(tokenized_dataset[\"input_ids\"][99])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZtw-lkSHiPN",
        "outputId": "58f3bac1-6310-4a5a-9328-c4c8601fcee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In that regard, the column “white found decent reply” shows the percentage of\n",
            "games in which white found one of the moves included under the “branches better\n",
            "than -2” column. After all, we are hoping that our opponent is going to make a bad\n",
            "move, so it is good to know how frequently players have made mistakes facing the\n",
            "same position. The column labelled “branches frequency >10%” indicates, following\n",
            "Black’s move 7, the number of White replies at move 8 which were used in more than\n",
            "10% of the games in the database. Again, this gets at the issue of how many lines we\n",
            "will need to extend our opening book to move 9 or beyond. To be safe, it is best to\n",
            "consider White’s moves if WZebra rates them highly or if they have been used fre-\n",
            "quently in actual play.\n",
            "Given all this information, we can start to get some idea of the advantages and\n",
            "disadvantages of each choice at move 7. The move which WZebra rates the highest,\n",
            "g4, is played in a majority of games. There is only one good choice for White at 8, and\n",
            "it is natural to expect that any expert player would know this move. Thus, playing g4\n",
            "triggers a semiautomatic response at 8, and we need to research the opening from\n",
            "move 9 onward.\n",
            "By contrast, 7. c6 is given a slightly lower evaluation, but has been played only\n",
            "3% of the time, which should mean that most players will not have studied it exten-\n",
            "sively. White has 2 reasonable choices at 8 and you should expect most of your oppo-\n",
            "According to the above context, answer the following question.\n",
            "What is the name of the column that shows the percentage of games in which white found one of the moves included under the \"branches better than -2\"?\n",
            "[688, 326, 2743, 13, 253, 5084, 773, 11300, 1119, 12524, 12252, 668, 2722, 253, 7155, 273, 187, 39464, 275, 534, 3168, 1119, 581, 273, 253, 9727, 2908, 762, 253, 773, 1288, 37426, 1805, 187, 14644, 428, 19, 668, 5084, 15, 2732, 512, 13, 359, 403, 11525, 326, 776, 16871, 310, 1469, 281, 1056, 247, 3076, 187, 15106, 13, 594, 352, 310, 1175, 281, 871, 849, 7208, 3773, 452, 1160, 16503, 10268, 253, 187, 18941, 1899, 15, 380, 5084, 27214, 773, 1288, 37426, 4294, 2239, 740, 6, 668, 6492, 13, 1563, 187, 15383, 457, 84, 2118, 818, 13, 253, 1180, 273, 5219, 32114, 387, 2118, 854, 534, 497, 908, 275, 625, 685, 187, 740, 6, 273, 253, 3958, 275, 253, 5447, 15, 10036, 13, 436, 4850, 387, 253, 2523, 273, 849, 1142, 3104, 359, 187, 9846, 878, 281, 9017, 776, 5909, 1984, 281, 2118, 898, 390, 4457, 15, 1916, 320, 4999, 13, 352, 310, 1682, 281, 187, 15603, 5219, 457, 84, 9727, 604, 411, 59, 25656, 4142, 731, 4122, 390, 604, 597, 452, 644, 908, 4107, 14, 187, 371, 1574, 275, 4588, 1132, 15, 187, 15768, 512, 436, 1491, 13, 359, 476, 1265, 281, 755, 690, 2934, 273, 253, 11361, 285, 187, 3431, 11402, 1131, 273, 1016, 4327, 387, 2118, 818, 15, 380, 2118, 534, 411, 59, 25656, 4142, 253, 4585, 13, 187, 72, 21, 13, 310, 4546, 275, 247, 5020, 273, 3958, 15, 1707, 310, 760, 581, 1175, 4327, 323, 5219, 387, 854, 13, 285, 187, 262, 310, 3626, 281, 1902, 326, 667, 6485, 4760, 651, 871, 436, 2118, 15, 3308, 13, 4882, 305, 21, 187, 39168, 7276, 247, 3300, 571, 307, 8977, 2380, 387, 854, 13, 285, 359, 878, 281, 2561, 253, 5909, 432, 187, 15106, 898, 47768, 15, 187, 3463, 4499, 13, 818, 15, 260, 23, 310, 1677, 247, 5777, 2406, 7103, 13, 533, 556, 644, 4546, 760, 187, 20, 6, 273, 253, 673, 13, 534, 943, 1599, 326, 954, 3773, 588, 417, 452, 5421, 352, 1021, 257, 14, 187, 84, 1242, 15, 5219, 556, 374, 5272, 10165, 387, 854, 285, 368, 943, 1902, 954, 273, 634, 1937, 80, 14, 187, 7130, 281, 253, 1840, 3634, 13, 3662, 253, 1563, 1953, 15, 187, 1276, 310, 253, 1416, 273, 253, 5084, 326, 2722, 253, 7155, 273, 3958, 275, 534, 3168, 1119, 581, 273, 253, 9727, 2908, 762, 253, 346, 1288, 37426, 1805, 685, 428, 19, 47579, 11300, 1119, 12524, 12252, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train - test split"
      ],
      "metadata": {
        "id": "5ENcS-qDIyvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "xvgLkepmJsBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(split_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Ev4hHgIJBR",
        "outputId": "16788611-3b64-4591-c930-b80c8748528f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 330\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 37\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGZCmghAJUfz",
        "outputId": "363fa335-22fb-4647-e6e8-c358c1ffd7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_dataset[17]['input']\n",
        "max_input_tokens = 1000\n",
        "max_output_tokens=2048\n",
        "# Tokenize\n",
        "input_ids = tokenizer.encode(\n",
        "      test_text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        ")\n",
        "\n",
        "# Generate\n",
        "device = base_model.device\n",
        "generated_tokens_with_prompt = base_model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVHsfMP8JaJE",
        "outputId": "26f8bbe6-863d-46fb-ba65-91a26dd62b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "print(generated_text_with_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tladjlZ0LFt-",
        "outputId": "cf7ad6ec-92c8-4d79-b2b3-ba51e0f70681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Read the following context and answer the question.\\nContext: gram 12-4), which accomplishes the goal of forcing Black to play to the south. Fur-\\nther, White still has two moves to the east at h5 (or h4) and g4.\\nDiagram 12-1\\nDiagram 12-2\\nDiagram 12-3\\nWhite to move\\nBlack to move\\nWhite to move\\na\\n1\\nb\\n2\\nc\\n3\\nd\\n4\\ne\\n5\\nf\\n6\\ng\\n7\\nh\\n8\\na\\n1\\nb\\n2\\nc\\n3\\nd\\n4\\ne\\n5\\nf\\n6\\ng\\n7\\nh\\n8\\n1\\na\\n1\\nb\\n2\\nc\\n3\\nd\\n4\\ne\\n5\\nf\\n6\\ng\\n7\\nh\\n8\\n2\\na\\n1\\nb\\n2\\nc\\n3\\nd\\n4\\ne\\n5\\nf\\n6\\ng\\n7\\nh\\n8\\n1\\nIn fact, even if White let Black pass a couple of times,\\nand (from Diagram 12-1) played three moves in a row at\\ng6, h5, and g4, the result would still be favorable for\\nWhite, as Black would have to initiate play in the south.\\nAs this demonstrates, often a good way to take ad-\\nvantage of your opponent’s bad shapes is to think about\\nplaying several moves in a row with your opponent pass-\\ning. In Diagram 12-1, White could get in three good moves\\nby beginning with g6, but would only get two moves start-\\ning at g4.  Therefore, g6 is likely to work out better than\\ng4.\\nAdvanced midgame play\\n91\\n Next, consider Diagram 12-5. Compared to Diagram 12-1, the position is not as\\nfavorable to White, since Black has some moves available to the north. However,\\nWhite wants to follow the same basic strategy of forcing Black to initiate play to the\\nsouth. In Diagram 12-5, Black has an unbalanced edge, and if Black opens up the\\nsouth, White could eventually end up with a good move at b7.  As in Diagram 12-1,\\nWhite wants to try to gain some tempos by exploiting Black’s bad shape to the east,\\nQuestion: What is probably true about the writer of this book?\\nAnswer:\\n1. The writer of this book is a writer of this book.\\n2. The writer of this book is a writer of this book.\\n3. The writer of this book is a writer of this book.\\n4. The writer of this book is a writer of this book.\\n5. The writer of this book is a writer of this book.\\n6. The writer of this book is a writer of this book.\\n7. The writer of this book is a writer of this book.\\n8. The writer of this book is a writer of this book.\\n9. The writer of this book is a writer of this book.\\n10. The writer of this book is a writer of this book.\\n11. The writer of this book is a writer of this book.\\n12. The writer of this book is a writer of this book.\\n13. The writer of this book is a writer of this book.\\n14. The writer of this book is a writer of this book.\\n15. The writer of this book is a writer of this book.\\n16. The writer of this book is a writer of this book.\\n17. The writer of this book is a writer of this book.\\n18. The writer of this book is a writer of this book.\\n19. The writer of this book is a writer of this book.\\n20. The writer of this book is a writer of this book.\\n21. The writer of this book is a writer of this book.\\n22. The writer of this book is a writer of this book.\\n23. The writer of this book is a writer of this book.\\n24. The writer of this book is a writer of this book.\\n25. The writer of this book is a writer of this book.\\n26. The writer of this book is a writer of this book.\\n27. The writer of this book is a writer of this book.\\n28. The writer of this book is a writer of this book.\\n29. The writer of this book is a writer of this book.\\n30. The writer of this book is a writer of this book.\\n31. The writer of this book is a writer of this book.\\n32. The writer of this book is a writer of this book.\\n33. The writer of this book is a writer of this book.\\n34. The writer of this book is a writer of this book.\\n35. The writer of this book is a writer of this book.\\n36. The writer of this book is a writer of this book.\\n37. The writer of this book is a writer of this book.\\n38. The writer of this book is a writer of this book.\\n39. The writer of this book is a writer of this book.\\n40. The writer of this book is a writer of this book.\\n41. The writer of this book is a writer of this book.\\n42. The writer of this book is a writer of this book.\\n43. The writer of this book is a writer of this book.\\n44. The writer of this book is a writer of this book.\\n45. The writer of this book is a writer of this book.\\n46. The writer of this book is a writer of this book.\\n47. The writer of this book is a writer of this book.\\n48. The writer of this book is a writer of this book.\\n49. The writer of this book is a writer of this book.\\n50. The writer of this book is a writer of this book.\\n51. The writer of this book is a writer of this book.\\n52. The writer of this book is a writer of this book.\\n53. The writer of this book is a writer of this book.\\n54. The writer of this book is a writer of this book.\\n55. The writer of this book is a writer of this book.\\n56. The writer of this book is a writer of this book.\\n57. The writer of this book is a writer of this book.\\n58. The writer of this book is a writer of this book.\\n59. The writer of this book is a writer of this book.\\n60. The writer of this book is a writer of this book.\\n61. The writer of this book is a writer of this book.\\n62. The writer of this book is a writer of this book.\\n63. The writer of this book is a writer of this book.\\n64. The writer of this book is a writer of this book.\\n65. The writer of this book is a writer of this book.\\n66. The writer of this book is a writer of this book.\\n67. The writer of this book is a writer of this book.\\n68. The writer of this book is a writer of this book.\\n69. The writer of this book is a writer of this book.\\n70. The writer of this book is a writer of this book.\\n71. The writer of this book is a writer of this book.\\n72. The writer of this book is a writer of this book.\\n73. The writer of this book is a writer of this book.\\n74. The writer of this book is a writer of this book.\\n75. The writer of this book is a writer of this book.\\n76. The writer of this book is a writer of this book.\\n77. The writer of this book is a writer of this book.\\n78. The writer of this book is a writer of this book.\\n79. The writer of this book is a writer of this book.\\n80. The writer of this book is a writer of this book.\\n81. The writer of this book is a writer of this book.\\n82. The writer of this book is a writer of this book.\\n83. The writer of this book is a writer of this book.\\n84. The writer of this book is a writer of this book.\\n85. The writer of this book is a writer of this book.\\n86. The writer of this book is a writer of this book.\\n87. The writer of this book is a writer of this book.\\n88. The writer of this book is a writer of this book.\\n89. The writer of this book is a writer of this book.\\n90. The writer of this book is a writer of this book.\\n91. The writer of this book is a writer of this book.\\n92. The writer of this book is a writer of this book.\\n93. The writer of this book is a writer of this book.\\n94. The writer of this book is a writer of this book.\\n95. The writer of this book is a writer of this book.\\n96. The writer of this book is a writer of this book.\\n97. The writer of this book is a writer of this book.\\n98. The writer of this book is a writer of this book.\\n99. The writer of this book is a writer of this book.\\n100. The writer of this book is a writer of this book.\\n101. The writer of this book is a writer of this book.\\n102. The writer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip the prompt\n",
        "generated_text_answer = generated_text_with_prompt[0][len(test_text):]\n",
        "\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"\\nCorrect answer from docs: {test_dataset[0]['output']}\")\n",
        "print(\"\\nModel's answer: \")\n",
        "print(generated_text_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kMBjkQjLVIS",
        "outputId": "fe759083-12f8-4cb1-bf09-ee06e1232357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): Read the following context and answer the question.\n",
            "Context: gram 12-4), which accomplishes the goal of forcing Black to play to the south. Fur-\n",
            "ther, White still has two moves to the east at h5 (or h4) and g4.\n",
            "Diagram 12-1\n",
            "Diagram 12-2\n",
            "Diagram 12-3\n",
            "White to move\n",
            "Black to move\n",
            "White to move\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "1\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "2\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "1\n",
            "In fact, even if White let Black pass a couple of times,\n",
            "and (from Diagram 12-1) played three moves in a row at\n",
            "g6, h5, and g4, the result would still be favorable for\n",
            "White, as Black would have to initiate play in the south.\n",
            "As this demonstrates, often a good way to take ad-\n",
            "vantage of your opponent’s bad shapes is to think about\n",
            "playing several moves in a row with your opponent pass-\n",
            "ing. In Diagram 12-1, White could get in three good moves\n",
            "by beginning with g6, but would only get two moves start-\n",
            "ing at g4.  Therefore, g6 is likely to work out better than\n",
            "g4.\n",
            "Advanced midgame play\n",
            "91\n",
            " Next, consider Diagram 12-5. Compared to Diagram 12-1, the position is not as\n",
            "favorable to White, since Black has some moves available to the north. However,\n",
            "White wants to follow the same basic strategy of forcing Black to initiate play to the\n",
            "south. In Diagram 12-5, Black has an unbalanced edge, and if Black opens up the\n",
            "south, White could eventually end up with a good move at b7.  As in Diagram 12-1,\n",
            "White wants to try to gain some tempos by exploiting Black’s bad shape to the east,\n",
            "Question: What is probably true about the writer of this book?\n",
            "Answer:\n",
            "\n",
            "Correct answer from docs: He is an expert at the game\n",
            "\n",
            "Model's answer: \n",
            "\n",
            "1. The writer of this book is a writer of this book.\n",
            "2. The writer of this book is a writer of this book.\n",
            "3. The writer of this book is a writer of this book.\n",
            "4. The writer of this book is a writer of this book.\n",
            "5. The writer of this book is a writer of this book.\n",
            "6. The writer of this book is a writer of this book.\n",
            "7. The writer of this book is a writer of this book.\n",
            "8. The writer of this book is a writer of this book.\n",
            "9. The writer of this book is a writer of this book.\n",
            "10. The writer of this book is a writer of this book.\n",
            "11. The writer of this book is a writer of this book.\n",
            "12. The writer of this book is a writer of this book.\n",
            "13. The writer of this book is a writer of this book.\n",
            "14. The writer of this book is a writer of this book.\n",
            "15. The writer of this book is a writer of this book.\n",
            "16. The writer of this book is a writer of this book.\n",
            "17. The writer of this book is a writer of this book.\n",
            "18. The writer of this book is a writer of this book.\n",
            "19. The writer of this book is a writer of this book.\n",
            "20. The writer of this book is a writer of this book.\n",
            "21. The writer of this book is a writer of this book.\n",
            "22. The writer of this book is a writer of this book.\n",
            "23. The writer of this book is a writer of this book.\n",
            "24. The writer of this book is a writer of this book.\n",
            "25. The writer of this book is a writer of this book.\n",
            "26. The writer of this book is a writer of this book.\n",
            "27. The writer of this book is a writer of this book.\n",
            "28. The writer of this book is a writer of this book.\n",
            "29. The writer of this book is a writer of this book.\n",
            "30. The writer of this book is a writer of this book.\n",
            "31. The writer of this book is a writer of this book.\n",
            "32. The writer of this book is a writer of this book.\n",
            "33. The writer of this book is a writer of this book.\n",
            "34. The writer of this book is a writer of this book.\n",
            "35. The writer of this book is a writer of this book.\n",
            "36. The writer of this book is a writer of this book.\n",
            "37. The writer of this book is a writer of this book.\n",
            "38. The writer of this book is a writer of this book.\n",
            "39. The writer of this book is a writer of this book.\n",
            "40. The writer of this book is a writer of this book.\n",
            "41. The writer of this book is a writer of this book.\n",
            "42. The writer of this book is a writer of this book.\n",
            "43. The writer of this book is a writer of this book.\n",
            "44. The writer of this book is a writer of this book.\n",
            "45. The writer of this book is a writer of this book.\n",
            "46. The writer of this book is a writer of this book.\n",
            "47. The writer of this book is a writer of this book.\n",
            "48. The writer of this book is a writer of this book.\n",
            "49. The writer of this book is a writer of this book.\n",
            "50. The writer of this book is a writer of this book.\n",
            "51. The writer of this book is a writer of this book.\n",
            "52. The writer of this book is a writer of this book.\n",
            "53. The writer of this book is a writer of this book.\n",
            "54. The writer of this book is a writer of this book.\n",
            "55. The writer of this book is a writer of this book.\n",
            "56. The writer of this book is a writer of this book.\n",
            "57. The writer of this book is a writer of this book.\n",
            "58. The writer of this book is a writer of this book.\n",
            "59. The writer of this book is a writer of this book.\n",
            "60. The writer of this book is a writer of this book.\n",
            "61. The writer of this book is a writer of this book.\n",
            "62. The writer of this book is a writer of this book.\n",
            "63. The writer of this book is a writer of this book.\n",
            "64. The writer of this book is a writer of this book.\n",
            "65. The writer of this book is a writer of this book.\n",
            "66. The writer of this book is a writer of this book.\n",
            "67. The writer of this book is a writer of this book.\n",
            "68. The writer of this book is a writer of this book.\n",
            "69. The writer of this book is a writer of this book.\n",
            "70. The writer of this book is a writer of this book.\n",
            "71. The writer of this book is a writer of this book.\n",
            "72. The writer of this book is a writer of this book.\n",
            "73. The writer of this book is a writer of this book.\n",
            "74. The writer of this book is a writer of this book.\n",
            "75. The writer of this book is a writer of this book.\n",
            "76. The writer of this book is a writer of this book.\n",
            "77. The writer of this book is a writer of this book.\n",
            "78. The writer of this book is a writer of this book.\n",
            "79. The writer of this book is a writer of this book.\n",
            "80. The writer of this book is a writer of this book.\n",
            "81. The writer of this book is a writer of this book.\n",
            "82. The writer of this book is a writer of this book.\n",
            "83. The writer of this book is a writer of this book.\n",
            "84. The writer of this book is a writer of this book.\n",
            "85. The writer of this book is a writer of this book.\n",
            "86. The writer of this book is a writer of this book.\n",
            "87. The writer of this book is a writer of this book.\n",
            "88. The writer of this book is a writer of this book.\n",
            "89. The writer of this book is a writer of this book.\n",
            "90. The writer of this book is a writer of this book.\n",
            "91. The writer of this book is a writer of this book.\n",
            "92. The writer of this book is a writer of this book.\n",
            "93. The writer of this book is a writer of this book.\n",
            "94. The writer of this book is a writer of this book.\n",
            "95. The writer of this book is a writer of this book.\n",
            "96. The writer of this book is a writer of this book.\n",
            "97. The writer of this book is a writer of this book.\n",
            "98. The writer of this book is a writer of this book.\n",
            "99. The writer of this book is a writer of this book.\n",
            "100. The writer of this book is a writer of this book.\n",
            "101. The writer of this book is a writer of this book.\n",
            "102. The writer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "GJInsW3IL8W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# number of epoch\n",
        "max_steps = 100\n",
        "\n",
        "# Save model to this direction\n",
        "trained_model_name = f\"othello_docs_{max_steps}_steps_v2\"\n",
        "output_dir = trained_model_name\n",
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  save_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    # model_flops=model_flops,\n",
        "    # total_steps=max_steps,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "\n",
        "training_output = trainer.train()\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "wNT91t0YLVay",
        "outputId": "c75a4395-5c58-4206-8def-cb04616a224b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 27:58, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: othello_docs_100_steps_v2/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output for download\n",
        "folder = f'/content/{trained_model_name}'\n",
        "!zip -r /content/trained_model.zip \"$folder\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFN_xT0SM0YG",
        "outputId": "da8d1526-2636-49fd-a87a-108a6433a9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/othello_docs_100_steps_v2/ (stored 0%)\n",
            "  adding: content/othello_docs_100_steps_v2/runs/ (stored 0%)\n",
            "  adding: content/othello_docs_100_steps_v2/runs/Oct06_10-33-58_883819d1b179/ (stored 0%)\n",
            "  adding: content/othello_docs_100_steps_v2/runs/Oct06_10-33-58_883819d1b179/events.out.tfevents.1728210839.883819d1b179.169.2 (deflated 67%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/ (stored 0%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/generation_config.json (deflated 23%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/trainer_state.json (deflated 82%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/model.safetensors (deflated 18%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/optimizer.pt (deflated 32%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/scheduler.pt (deflated 56%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/config.json (deflated 47%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/training_args.bin (deflated 51%)\n",
            "  adding: content/othello_docs_100_steps_v2/checkpoint-100/rng_state.pth (deflated 24%)\n",
            "  adding: content/othello_docs_100_steps_v2/final/ (stored 0%)\n",
            "  adding: content/othello_docs_100_steps_v2/final/generation_config.json (deflated 23%)\n",
            "  adding: content/othello_docs_100_steps_v2/final/model.safetensors (deflated 18%)\n",
            "  adding: content/othello_docs_100_steps_v2/final/config.json (deflated 47%)\n",
            "  adding: content/othello_docs_100_steps_v2/final/training_args.bin (deflated 51%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test / Evaluate"
      ],
      "metadata": {
        "id": "1QXBwvN1Qob5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "finetuned_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xoLolqPO6Qz",
        "outputId": "c7cccf51-eee9-4097-fcbe-3671c8da6659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXSdpaAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(test_question, model):\n",
        "\n",
        "    # Tokenize\n",
        "    input_ids = tokenizer.encode(\n",
        "          test_question,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    device = model.device\n",
        "    generated_tokens_with_prompt = model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)\n",
        "\n",
        "    # Decode\n",
        "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "    # Strip the prompt\n",
        "    generated_text_answer = generated_text_with_prompt[0][len(test_question):]\n",
        "    return generated_text_answer"
      ],
      "metadata": {
        "id": "zA2CsGDcQ2vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[27]['input']\n",
        "test_answer = test_dataset[27]['output']\n",
        "predicted_text = generate_output(test_question, finetuned_model)\n",
        "base_predicted_text = generate_output(test_question, base_model)\n",
        "\n",
        "print('Question:')\n",
        "print(test_question)\n",
        "print(\"--------------------------------------\")\n",
        "print('Actual Completion:')\n",
        "print(test_answer)\n",
        "print(\"--------------------------------------\")\n",
        "print('Fine-tuned prediction')\n",
        "print(predicted_text)\n",
        "print(\"--------------------------------------\")\n",
        "print('Base prediction:')\n",
        "print(base_predicted_text)\n",
        "print(\"--------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-OT6WKiQ93r",
        "outputId": "ae05fd9d-8f8a-41a2-f4da-e80df0b2a74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "Read the following article and answer the question.\n",
            "Article: want to play e8 in order for White to exploit a good potential move at g5. In short, the\n",
            "shapes in Diagram 12-7 cry out for White to move to e8.\n",
            "Now consider Diagram 12-9. Here too, e8 jumps out as the place White wants to\n",
            "play, but for the moment he can not, because the e-column is entirely black. In the\n",
            "actual game, Tamenori played f2! (Diagram 12-10), threatening e8 on his next move.\n",
            "The game continued Black e2, White c1, Black d1, resulting in the position shown in\n",
            "Diagram 12-7.\n",
            "In Diagram 12-9, Tamenori came up with a clever\n",
            "way to set up a move to e8, but even a not-so-clever\n",
            "move such as c2 would have been reasonable. In the\n",
            "midgame, it is important to be able to look at a posi-\n",
            "tion and quickly identify the “hot spots” that each side\n",
            "wants to play to. If you can quickly find the right move\n",
            "in positions such as Diagram 12-1 and Diagram 12-7,\n",
            "then you should eventually be able to find good moves\n",
            "in situations such as Diagram 12-5 and Diagram 12-9.\n",
            "Diagram 12-10\n",
            "Black to move\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "1\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "a\n",
            "1\n",
            "b\n",
            "2\n",
            "c\n",
            "3\n",
            "d\n",
            "4\n",
            "e\n",
            "5\n",
            "f\n",
            "6\n",
            "g\n",
            "7\n",
            "h\n",
            "8\n",
            "1\n",
            "Advanced midgame play\n",
            "93\n",
            "Bad shape: look before you leap\n",
            "In Diagram 12-11, Black is in danger of running out of moves. If it were Black’s\n",
            "turn to move, pretty much the only choice would be to play a6. White can therefore\n",
            "put pressure on Black by playing a2 (Diagram 12-12). Now if Black plays a6, White\n",
            "Question: What is the passage mainly about?\n",
            "Answer:\n",
            "--------------------------------------\n",
            "Actual Completion:\n",
            "To play chess\n",
            "--------------------------------------\n",
            "Fine-tuned prediction\n",
            "The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "\n",
            "--------------------------------------\n",
            "Base prediction:\n",
            "The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "Answer:The passage is about the passage.\n",
            "Question: What is the passage about?\n",
            "\n",
            "--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "zjPtTz2aRpf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_predicted_text_list = []\n",
        "actual_test_list = []\n",
        "base_predicted_text_list = []\n",
        "for i in range(len(test_dataset)):\n",
        "    # get prompt\n",
        "    test_q = test_dataset[i]['input']\n",
        "    # get completion\n",
        "    completion_q = test_dataset[i]['output']\n",
        "    # predictions\n",
        "    predicted_text = generate_output(test_question, finetuned_model)\n",
        "    base_predicted_text = generate_output(test_question, base_model)\n",
        "    # collect\n",
        "    actual_test_list.append(completion_q)\n",
        "    tuned_predicted_text_list.append(predicted_text)\n",
        "    base_predicted_text_list.append(base_predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXgqIWuuRaUL",
        "outputId": "e6c33cb3-5a96-4e4c-b54f-f98f59fc074c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3awjGDYVSFAe",
        "outputId": "60d0de4b-2b69-4c25-b8d8-0eb4e87d3b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import evaluate\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "results = bleu.compute(predictions=base_predicted_text_list, references=actual_test_list)\n",
        "print(\"Base Model Predictions Results\")\n",
        "print(results)\n",
        "\n",
        "results = bleu.compute(predictions=tuned_predicted_text_list, references=actual_test_list)\n",
        "print(\"Fine-tuned Model Predictions Results\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbo2xmOjR_wD",
        "outputId": "fdbc7c50-00b0-4910-f2a0-84d9a645adbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model Predictions Results\n",
            "{'bleu': 0.0, 'precisions': [0.0028531833374093097, 0.0008171269815329302, 0.00024572037021869115, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 106.66956521739131, 'translation_length': 12267, 'reference_length': 115}\n",
            "Fine-tuned Model Predictions Results\n",
            "{'bleu': 0.0, 'precisions': [0.0028531833374093097, 0.0008171269815329302, 0.00024572037021869115, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 106.66956521739131, 'translation_length': 12267, 'reference_length': 115}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqKnkO6nV7PG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}